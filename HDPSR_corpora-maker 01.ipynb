{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported.\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities, matutils\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "df_train = pd.read_csv(\"D:/_Barq/HDPSR/train.csv\", encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv('D:/_Barq/HDPSR/test.csv', encoding=\"ISO-8859-1\")\n",
    "df_pro_desc = pd.read_csv('D:/_Barq/HDPSR/product_descriptions.csv')\n",
    "df_attribute = pd.read_csv(\"D:/_Barq/HDPSR/attributes.csv\", sep=\",\", encoding=\"ISO-8859-1\")\n",
    "df_attribute['attribute'] = df_attribute.name + ' ' + df_attribute.value\n",
    "df_attribute = df_attribute.drop(['name', 'value'], axis=1)\n",
    "df_attribute = df_attribute.groupby(['product_uid'])['attribute'].apply( \\\n",
    "                    lambda x: ' '.join(x.astype('str'))).reset_index()\n",
    "\n",
    "stops = set(stopwords.words(\"english\")) \\\n",
    "        | set('bullet b c e f g h j k l m n o p q r s t u v w x y z'.split())\n",
    "def cleaner(info,stops):\n",
    "        info = re.sub(r'([a-z])([A-Z])',r'\\1 \\2',info) \n",
    "        info = re.sub(r'[^a-zA-Z]',' ',info) \n",
    "        info = info.lower().split() \n",
    "        info = [word for word in info if not word in stops] \n",
    "        return \" \".join( info )\n",
    "    \n",
    "df_pro_desc.product_description = [cleaner(info,stops) \\\n",
    "                                   for info in df_pro_desc.product_description.astype('str')]\n",
    "df_train.product_title = [cleaner(info,stops) \\\n",
    "                                   for info in df_train.product_title.astype('str')]\n",
    "df_test.product_title = [cleaner(info,stops) \\\n",
    "                                   for info in df_test.product_title.astype('str')]\n",
    "df_attribute.attribute = [cleaner(info,stops) \\\n",
    "                                   for info in df_attribute.attribute.astype('str')]\n",
    "\n",
    "print('All libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train = df_train.shape[0]\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')\n",
    "df_all = pd.merge(df_all, df_attribute, how='left', on='product_uid')\n",
    "\n",
    "def str_stemmer(s):\n",
    "\treturn \" \".join([stemmer.stem(word) for word in s.lower().split()])\n",
    "\n",
    "def str_common_word(str1, str2):\n",
    "\treturn sum(int(str2.find(word)>=0) for word in str1.split())\n",
    "\n",
    "df_all['search_term'] = df_all['search_term'].map(lambda x:str_stemmer(x))\n",
    "df_all['product_title'] = df_all['product_title'].map(lambda x:str_stemmer(x))\n",
    "df_all['product_description'] = df_all['product_description'].map(lambda x:str_stemmer(x))\n",
    "df_all['attribute'] = df_all['attribute'].map(lambda x:str_stemmer(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "descriptions = df_all.product_description[:num_train]\n",
    "\n",
    "terms = [[term for term in all_term if term not in stoplist] \n",
    "        for all_term in all_terms]\n",
    "\n",
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for term in terms:\n",
    "    for token in term:\n",
    "        frequency[token] += 1\n",
    "\n",
    "terms = [[token for token in term if frequency[token] > 1]\n",
    "          for term in terms]\n",
    "\n",
    "dictionary = corpora.Dictionary(terms)\n",
    "dictionary.save('D:/_Barq/HDPSR/Discription.dict') # store the dictionary, for future reference\n",
    "\n",
    "corpus = [dictionary.doc2bow(term) for term in terms]\n",
    "corpora.MmCorpus.serialize('D:/_Barq/HDPSR/Discription.mm', corpus) # store to disk, for later use\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(20514 unique tokens: ['baits', 'condenser', 'lillington', 'sub', 'gala']...)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('D:/_Barq/HDPSR/train.csv',sep=\",\", encoding=\"ISO-8859-1\")\n",
    "test = pd.read_csv('D:/_Barq/HDPSR/test.csv',sep=\",\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "train_df = train[['product_uid', 'product_title']]\n",
    "test_df = test[['product_uid', 'product_title']]\n",
    "titles_df = train_df.append(test_df)\n",
    "titles_df = titles_df.drop_duplicates()\n",
    "\n",
    "# Split joint capital character words\n",
    "titles = [re.sub(r'([a-z])([A-Z])',r'\\1 \\2',title) \n",
    "                for title in titles_df.product_title]\n",
    "# Remove some unnecessary chars and split titles\n",
    "all_title_terms = (re.findall('([a-z]+)', title.lower()) for title in titles)\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in at from or on with '\n",
    "                'be am is are was were '\n",
    "                'it he she they you we ' \n",
    "                'will would should shall may might must '\n",
    "                'do does ' \n",
    "                'my her his them our us '\n",
    "                'mine hers his yours theirs its '\n",
    "                'not only also which that this these those '\n",
    "                '1 2 3 4 5 6 7 8 9 0 '\n",
    "                'a b c d e f g h i j k l m n o p q r s t u v w x y z xx'\n",
    "                'where when who why'.split())\n",
    "\n",
    "title_terms = [[term for term in all_term if term not in stoplist] \n",
    "        for all_term in all_title_terms]\n",
    "\n",
    "dictionary = corpora.Dictionary(title_terms)\n",
    "dictionary.save('D:/_Barq/HDPSR/Titles.dict') # store the dictionary, for future reference\n",
    "\n",
    "corpus = [dictionary.doc2bow(term) for term in title_terms]\n",
    "corpora.MmCorpus.serialize('D:/_Barq/HDPSR/Titles.mm', corpus) # store to disk, for later use\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(29506 unique tokens: ['baits', 'torques', 'sub', 'chiropractic', 'mini']...)\n"
     ]
    }
   ],
   "source": [
    "attributes_df = pd.read_csv('D:/_Barq/HDPSR/attributes.csv',sep=\",\", encoding=\"ISO-8859-1\")\n",
    "attributes_df['name_value'] = attributes_df.name + ' ' + attributes_df.value\n",
    "attributes_df = attributes_df.drop(['name', 'value'], axis=1)\n",
    "attributes_df = attributes_df.groupby(['product_uid'])['name_value'\n",
    "                ].apply(lambda x: ' '.join(x.astype('str'))).reset_index()\n",
    "\n",
    "# Split joint capital character words\n",
    "attributes = [re.sub(r'([a-z])([A-Z])',r'\\1 \\2',attribute) \n",
    "                for attribute in attributes_df.name_value]\n",
    "# Remove some unnecessary chars and split titles\n",
    "all_attribute_terms = (re.findall('([a-z]+)', attribute.lower()) for attribute in attributes)\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in at from or on with '\n",
    "                'be am is are was were '\n",
    "                'it he she they you we ' \n",
    "                'will would should shall may might must '\n",
    "                'do does ' \n",
    "                'my her his them our us '\n",
    "                'mine hers his yours theirs its '\n",
    "                'not only also which that this these those '\n",
    "                '1 2 3 4 5 6 7 8 9 0 '\n",
    "                'a b c d e f g h i j k l m n o p q r s t u v w x y z xx'\n",
    "                'where when who why'\n",
    "                'bullet'.split())\n",
    "\n",
    "attribute_terms = [[term for term in all_term if term not in stoplist] \n",
    "        for all_term in all_attribute_terms]\n",
    "\n",
    "dictionary = corpora.Dictionary(attribute_terms)\n",
    "dictionary.save('D:/_Barq/HDPSR/Attributes.dict') # store the dictionary, for future reference\n",
    "\n",
    "corpus = [dictionary.doc2bow(term) for term in title_terms]\n",
    "corpora.MmCorpus.serialize('D:/_Barq/HDPSR/Attributes.mm', corpus) # store to disk, for later use\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
